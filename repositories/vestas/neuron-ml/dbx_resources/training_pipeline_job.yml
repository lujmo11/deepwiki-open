resources:
  jobs:
    neuron_training_job:
      name: neuron_training_job
      max_concurrent_runs: 10
      tasks:
        - task_key: neuron_training_task
          job_cluster_key: job_cluster
          python_wheel_task:
            package_name: neuron
            entry_point: train
            parameters:
              ["--train-config-path", "training_run_config_example.yaml"]
          libraries:
            - whl: ../dist/*.whl
      job_clusters:
        - job_cluster_key: job_cluster
          new_cluster:
            spark_version: 16.0.x-gpu-ml-scala2.12
            node_type_id: Standard_NC4as_T4_v3
            autoscale:
              min_workers: 1
              max_workers: 12
            spark_env_vars:
              COMPUTE_ENVIRONMENT: databricks
              PIP_INDEX_URL: "{{secrets/ml-product-scope/pipindexurl}}"
              GIT_COMMIT_HASH: ${bundle.git.commit}

    neuron_eval_job:
      name: neuron_eval_job
      max_concurrent_runs: 10
      tasks:
        - task_key: neuron_eval_task
          job_cluster_key: eval_cluster
          python_wheel_task:
            package_name: neuron
            entry_point: eval
            parameters:
              [
                "--train-config-path",
                "training_run_config_example.yaml",
                "--mlflow-run-id",
                "placeholder",
              ]
          libraries:
            - whl: ../dist/*.whl
      job_clusters:
        - job_cluster_key: eval_cluster
          new_cluster:
            spark_version: 16.0.x-gpu-ml-scala2.12
            node_type_id: Standard_NC4as_T4_v3
            num_workers: 0
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: local[*]
            custom_tags:
              ResourceClass: SingleNode
            spark_env_vars:
              COMPUTE_ENVIRONMENT: databricks
              PIP_INDEX_URL: "{{secrets/ml-product-scope/pipindexurl}}"
              GIT_COMMIT_HASH: ${bundle.git.commit}
